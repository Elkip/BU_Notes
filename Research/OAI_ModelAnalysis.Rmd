# Model Analysis in R

This prediction analysis (objective 2) will use all baseline OAI subjects to see if the TKR subgroups identified in objective 1 are predictable using baseline information. This analysis will be done using the set of the most clinically meaningful clusters identified in objective 1, by random forests. To avoid biases due to study withdrawal or death before TKR these will be modeled as separate multinomial outcomes. So, the outcome has 3 levels (alive and free of TKR (reference), withdrew from study without TKR, and death (without TKR) in addition to one level for each of the clusters identified in objective 1. Relevant interactions in the covariates detected in the random forest analysis will be considered for inclusion in the multinomial logistic regression model in addition to the covariates and any other relevant predictors.

```{r, warning=FALSE, message=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
library(tidyverse)
library(nnet) # For multinomial model
library(ggeffects) # Plotting for regression models
options(scipen = 999) # No scientific notation
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
if (DATAPATH == "" ) stop( "Please add datapath to OAI_DATA" )

if(!exists('data_full')) {
  # LoadData must be in working directory
  source("OAI_LoadData.R", chdir = T) 
  data_full <- getCompleteData(DATAPATH, cluster_selection)
}

data_best <- data_full %>% select(c(predictors_best, "EVNT"))
```

Observations with missing predictors will be removed from the data

```{r omitNaBest}
data_best <- na.omit(data_best)
```

## Best Model Analysis

The backwards step-wise selection gives the lowest AIC and best deviance, I will proceed using the parameters selected from that model.

Note while my most coefficient estimates match SAS, the standard errors are extremely small and making every confidence intervals extremely narrow. This appears to be a result of scaling the many parameters. Scaling the parameters manually seems to resolve this issue, but they still don't exactly match the SAS output.

```{r best, message=FALSE}
eqtn_best <- formula(paste("EVNT ~ ", paste(predictors_best, collapse = " + "), "+ WEIGHT:HEIGHT"))

# data_best.s <- data.frame(EVNT = data_best$EVNT, AGE = scale(data_best$AGE), 
#     SEX = data_best$SEX, RACE_O = data_best$RACE_O, PASE = scale(data_best$PASE), 
#     WOMKP = scale(data_best$WOMKP), WOMSTF = scale(data_best$WOMSTF), 
#     HEIGHT = scale(data_best$HEIGHT), WEIGHT = scale(data_best$WEIGHT), 
#     V00WTMAXKG = scale(data_best$V00WTMAXKG), NSAID = data_best$NSAID,
#     P01OAGRD_Severe = data_best$P01OAGRD_Severe, P01OAGRD_Moderate = data_best$P01OAGRD_Moderate, 
#     P01OAGRD_Mild = data_best$P01OAGRD_Mild, P01OAGRD_Possible = data_best$P01OAGRD_Possible,
#     EDCV_HSDeg = data_best$EDCV_HSDeg, EDCV_GradDeg = data_best$EDCV_GradDeg,
#     EDCV_UGDeg = data_best$EDCV_UGDeg, CESD = scale(data_best$CESD), 
#     Surg_Inj_Hist = data_best$Surg_Inj_Hist)

mod_best <- multinom(eqtn_best, data=data_best, maxit = 1000)
summary(mod_best)$coefficients
```

```{r modelStats}
zscore <- summary(mod_best)$coefficients/summary(mod_best)$standard.errors
wscore <- (summary(mod_best)$coefficients)^2/(summary(mod_best)$standard.errors)^2
pvalue <- (1 - pnorm(abs(wscore), 0, 1)) * 2
pvalue
```

## Plotting the Model of Best Fit

The probabilities based on model outcome and confidence intervals for each level of outcome will be graphed with ggplot and ggeffects libraries.

From the ggeffects docs: ggpredict() can be described as conditional effects (i.e. these are conditioned on certain (reference) levels of factors), while ggemmeans() and ggeffect() return marginal means, since the effects are "marginalized" (or "averaged") over the levels of factors (or values of character vectors)

Plot of the probabilities of being in each outcome with other predictors held at reference:

```{r probPlot, message=FALSE, warning=FALSE}
lapply(predictors_best, function(x) plot(ggpredict(mod_best, terms=paste(x, "[all]"), ci.lvl = .90)))
```

The above probabilities are generated using the predict function for each predictor, where every other predictor is held at reference level. For example, to get the above probabilities of AGE=50 adjusted for other predictors:

```{r testProb}
new_data = data.frame(AGE=50, SEX="1", RACE_O="0", CESD=0, CEMPLOY_NW="0", PASE=0, WOMKP=0,WOMSTF=0, HEIGHT=0, WEIGHT=0, NSAID="0", P01OAGRD_Severe="0", P01OAGRD_Moderate="0", P01OAGRD_Mild="0", P01OAGRD_Possible="0", EDCV_HSDeg="0", EDCV_GradDeg="0", EDCV_UGDeg="0", V00WTMAXKG=0, Surg_Inj_Hist="0")
predict(mod_best, type="probs", newdata = new_data)
```

Plot of the probabilities of being in each outcome with other predictors held at average:

```{r probPlotAvgs}
avg_data <- data_best %>% 
  group_by(EVNT, SEX, RACE_O, NSAID, P01OAGRD_Severe, P01OAGRD_Moderate, P01OAGRD_Mild, P01OAGRD_Possible, EDCV_GradDeg, EDCV_UGDeg, EDCV_HSDeg, Surg_Inj_Hist) %>%
  summarise(AGE = mean(AGE), CESD = mean(CESD), PASE = mean(PASE), WOMKP = mean(WOMKP), WOMSTF = mean(WOMSTF), HEIGHT = mean(HEIGHT), WEIGHT = mean(WEIGHT), V00WTMAXKG = mean(V00WTMAXKG))

pred <- predict(mod_best, newdata = avg_data, type = "probs")
new_data <- cbind(avg_data, pred)

probPlot <- function(term) {
  new_data %>%
    pivot_longer((ncol(new_data) - max(as.numeric(new_data$EVNT))+1):ncol(new_data), names_to = "Cluster", values_to = "prob") %>%
    ggplot(aes(x = get(term), y = prob, color = Cluster)) +
    geom_line() +
    labs(x = term)
}

lapply(predictors_best, probPlot)
```

### Confidence Intervals

Plot confidence intervals of the odds of each outcome compared to the reference group for each predictor:

```{r oddsPlot, message=FALSE, warning=FALSE}
outcomeLabel <- function(i) {
  switch(as.character(i),
         '1' = {"No Event"},
         '2' = {"Drop Out"},
         '3' = {"Death"},
         (paste("Knee Replacement Cluster", i-3))
    )
}

odds <- exp(coef(mod_best))
odds <- odds[,-ncol(odds)]
param_ci <- confint(mod_best, level = .90)
param_ci <- param_ci[-nrow(param_ci),,]
odds_ci <- exp(param_ci)

p <- list()

# 2 plots for every outcome (reg and log odds)
for (i in seq(2,nrow(odds)*2,2)) {
  odds_df <- data.frame(boxOdds = odds[i/2,-1],
                        boxCILow = odds_ci[,1,i/2][-1],
                        boxCIHigh = odds_ci[,2,i/2][-1])
  ci_min = min(odds_df$boxCILow)
  ci_max = max(odds_df$boxCIHigh)
  
  plt <- ggplot(odds_df,  aes(x = boxOdds, y = predictors_best)) + 
    ylab("Predictor") +
    xlab("Odds Ratio") +
    ggtitle(paste("Odds of", outcomeLabel(i/2+1))) + 
    geom_point() + 
    geom_errorbarh(aes(xmax = boxCIHigh, xmin = boxCILow)) +
    scale_y_discrete(labels = predictors_best) +
    scale_x_continuous(breaks = seq(0, ci_max + .1, signif((ci_max - ci_min) / 10, 3)),
                       labels = seq(0, ci_max + .1, signif((ci_max - ci_min) / 10, 3)), 
                       limits = c(ci_min, ci_max)) +
    geom_vline(aes(xintercept = 1), size = .25, linetype = "dashed")  + 
    annotate("text", x=1 - (ci_max - ci_min) / 100, y=10, label="Odds = 1", angle=90)

    p[[i-1]] <- plt
    
    odds_df <- data.frame(boxOdds = log(odds[i/2,-1]),
                        boxCILow = log(odds_ci[,1,i/2][-1]),
                        boxCIHigh = log(odds_ci[,2,i/2][-1]))
    ci_min = log(ci_min)
    ci_max = log(ci_max)
    
    plt <- ggplot(odds_df,  aes(x = boxOdds, y = predictors_best)) + 
    ylab("Predictor") +
    xlab("Ln(Odds Ratio)") +
    ggtitle(paste("Log Odds of", outcomeLabel(i/2+1))) + 
    geom_point() + 
    geom_errorbarh(aes(xmax = boxCIHigh, xmin = boxCILow)) +
    scale_y_discrete(labels = predictors_best) +
    scale_x_continuous(breaks = seq(floor(ci_min), ci_max + .1, signif((ci_max - ci_min) / 10, 3)),
                       labels = seq(floor(ci_min), ci_max + .1, signif((ci_max - ci_min) / 10, 3)), 
                       limits = c(ci_min, ci_max)) +
    geom_vline(aes(xintercept = 0), size = .25, linetype = "dashed")  + 
    annotate("text", x=0 - (ci_max - ci_min) / 100, y=10, label="Log Odds = 0", angle=90)

  p[[i]] <- plt
}
p
```
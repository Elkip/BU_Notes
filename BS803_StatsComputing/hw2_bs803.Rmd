---
title: "HW2_BS803"
output: word_document
date: "2022-09-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("caret")
library("mlbench")
library("rpart")
```

**a. Save a new copy of the data w/o "id"**

```{r echo=FALSE}
data("BreastCancer")
boob <- BreastCancer[complete.cases(BreastCancer),2:ncol(BreastCancer)]
```

I also chose to omit 19 cases which were incomplete.

**b. Create 80% Training - 20% Testing Partitions**

```{r}
train_index <- createDataPartition(boob$Class, p=.8, list = F)
boob_train <- boob[train_index,]
boob_test <- boob[-train_index,]
```

**c. Train a CART ("rpart") model with Class as the outcome and all other variables as predictors**

```{r}
cart <- rpart(Class ~ ., data = boob_train)
rpart.plot::prp(cart, box.palette = "RdBu", shadow.col = "gray", nn = T)
```

First I try with the rpart function with all variables.

```{r}
cart_preds <- predict(cart, newdata = boob_test, type = "class")
table(predicted = cart_preds, actual = boob_test$Class)
```

This model produces a fair number of errors. I will try adjusting the model. I observe the predictors which are most influential and proceed with some additional options.

```{r}
# Tuning grid
grid <- expand.grid(.cp=c(0.01,0.05,0.1))
#Tuning sigma and C in SVM
# This will split our dataset into 10 parts, train in 9 and test on 1 and release for all
# combinations of train-test splits. 
# We will also repeat 3 times for each algorithm with different splits of the data into 10 groups
control <- trainControl(method="repeatedcv", number=10, repeats=3)

rpartFit <- train(Class ~ Cl.thickness + Cell.size + Cell.size + Bare.nuclei, data = boob_train, method = "rpart", metric="Accuracy", tuneGrid=grid, trControl = control)

rpart.plot::rpart.plot(rpartFit$finalModel)
```

This model is much more accurate, but hopefully not too over-fit to the data.

**d. Train another model using a different ML method of your choice**

I'm going to use a svm model

```{r}
svmControl <- trainControl(method="repeatedcv", number=10, repeats=3)
svmGrid <-  expand.grid(sigma = c(0.01,0.05,0.1,0.15),
                        C = c(1,3,5,7,9))

svmFit <- train(Class ~ Cl.thickness + Cell.size + Cell.size + Bare.nuclei, data = boob_train, method = "svmRadial", preProcess = c("scale", "center"), grid = svmGrid, trControl = svmControl)
```

**e. Compare the performance of your two models on the testing partition by (1) making predictions for the test observations (2) using confusion Matrix from caret**

```{r}
print("rpart predictions vs observations:")
rpartPreds <- predict(rpartFit, newdata =  boob_test)
table(predicted = rpartPreds, actual = boob_test$Class)
confusionMatrix(rpartPreds, boob_test$Class)
```

The rpart model incorrectly classifies 9 of the test set, with 5 false positives and 4 false negative

```{r}
print("SVM predictions vs observations:")
svmPreds <- predict(svmFit, newdata =  boob_test)
table(predicted = svmPreds, actual = boob_test$Class)
confusionMatrix(svmPreds, boob_test$Class)
```

Comparing SVM model with the test data produces 5 errors, 3 false positives and 2 false negatives.

Both models perform with 90%+ accuracy, but from the confidence intervals of accuracy we see the SVM model performs slightly better consistency as it has a smaller interval over a higher range of values. It also has a higher Specificity & sensitivity, though it may risk over-fitting the model.
